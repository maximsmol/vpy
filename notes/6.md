# Lexing Assignments

The next operation we will support is assignment expressions like `a = 123`. We add it to `test_lex.py` and start the implementation.

We begin with creating the lexing grammar for [identifiers (`a`)](https://docs.python.org/3.13/reference/lexical_analysis.html#identifiers) and [delimiters (`=`).](https://docs.python.org/3.13/reference/lexical_analysis.html#delimiters) Delimiters are easily implemented as operators before, but for identifiers we run into an issue—we need to use Unicode categories (e.g. `\p{Other_ID_Start}`) but the standard library `re` module does not support them. We will use the [`regex` module](https://pypi.org/project/regex/) instead. There are also a couple of tricky details.

The first is the normalization of the identifiers. In Unicode, some characters may have multiple equivalent encodings, which nevertheless produce different bytestrings:

```py
>>> unicodedata.normalize("NFD", "á").encode("utf-8")
b'a\xcc\x81'
>>> [ unicodedata.name(x) for x in unicodedata.normalize("NFD", "á") ]
['LATIN SMALL LETTER A', 'COMBINING ACUTE ACCENT']

>>> unicodedata.normalize("NFC", "á").encode("utf-8")
b'\xc3\xa1'
>>> [ unicodedata.name(x) for x in unicodedata.normalize("NFC", "á") ]
['LATIN SMALL LETTER A WITH ACUTE']

>>> unicodedata.normalize("NFKD", "á").encode("utf-8")
b'a\xcc\x81'
>>> [ unicodedata.name(x) for x in unicodedata.normalize("NFKD", "á") ]
['LATIN SMALL LETTER A', 'COMBINING ACUTE ACCENT']

>>> unicodedata.normalize("NFKC", "á").encode("utf-8")
b'\xc3\xa1'
>>> [ unicodedata.name(x) for x in unicodedata.normalize("NFKC", "á") ]
['LATIN SMALL LETTER A WITH ACUTE']
```

This means that naively comparing strings can lead to `á` (NFD, 3 bytes in UTF-8) and `á` (NFC, 2 bytes in UTF-8) referring to different variables. To address this, the spec specifies that identifiers should be converted to NFKC form. We cannot simply convert the `text` field of all tokens since we have to output a byte-wise identical string when unparsing, so we will need to deal with this later when using the token value for lookups and comparisons.

The second detail is that the Python spec actually uses [hardcoded values for `\p{Other_ID_Start}` and `\p{Other_ID_Continue}` taken from Unicode 15.1.0](https://www.unicode.org/Public/15.1.0/ucd/PropList.txt) rather than what is in the up-to-date Unicode databases, to avoid the set of valid identifiers changing with the Unicode specification. We will ignore this for now since it's a very niche concern.

The remaining lexing changes are straightforward. We will also add a cursor position note to `unknown token` exceptions coming out of the lexer for easier debugging.

# Parsing Assignments

Add `a = 123` to `test_parse.py`, then move on to the implementation.

First, we create node classes for each new node type: [`assignment_stmt`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-assignment_stmt) [`target_list`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-target_list) [`target`](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-target) and define `to_ast` for each of them. Then, modify `simple_stmt` to allow `assignment_stmt` as the child.

Now we need to deal with the fact that the first token, `a` looks like a variable lookup expression, and we cannot tell it is an assignment until 2 tokens later when we read `=`. We need to be able to try parsing a long sequence of tokens before possibly backtracking and trying to parse it differently. Note that an alternative would be to parse `a` as _both_ the start of an assignment statmenet and the start of an expression, and then decide later. This is roughly what non-hand-written parsers do to avoid backtracking. Also note that this case only needs two tokens of look-ahead (one for the optional whitespace, one for `=`), but we will support arbitrary lengths here.

We could try to use `untok`, but it turns out to be pretty difficult to track which tokens we have read in the case of nested backtracking checkpoints—both checkpoints would need to store a subset of the token stream. Instead, we can save the entire state of the parser and lexer classes and restore them if we decide to rollback. This is somewhat less efficient as the lexer has to re-scan the text, and it doesn't inherently support streaming parsing, but we don't care about those limitations for now. We might switch later.

We define the checkpoint method:

```py
@contextmanager
def checkpoint(self) -> Generator[None]:
    old = deepcopy(self)

    try:
        yield
    except Exception:
        self.lex = old.lex
        self.token_stack = old.token_stack
        self.children = old.children

        raise
```

To save the state, we construct a deep copy of the parser using [`copy.deepcopy`](https://docs.python.org/3/library/copy.html#copy.deepcopy) from the standard library. There is no helper to reset the fields of `self` to the saved values, so we have to do it by hand.

Since the only way to backtrack a checkpoint is with an exception, we will define one to represent a failed parse as opposed to programmer error:

```py
class ParseFailedError(RuntimeError): ...
```

Note that `expect` is the only places that used to raise on parse failure, and it is now the only places that uses `ParseFailedError`.

---

Now we take all our uses of `untok` and replace them with `checkpoint` so we don't have two ways of rolling back the parse. `opt` becomes rather verbose:

```py
def opt(self, typ: str) -> Token | None:
    try:
        with self.checkpoint():
            x = self.tok()
            if x.type != typ:
                raise ParseFailedError("opt")

            return x
    except ParseFailedError:
        return None
```

It first starts a checkpoint, then tries the parse and returns on success. If the parse fails, it raises an exception to rollback the checkpoint, then catches it to avoid crashing and returns `None` instead.

`a_expr`, on the other hand, is arguably improved. First note that we actuall have a bug, which was noted in a todo before:

```py
# todo(maximsmol): fix eating this whitespace even if there is no +
_ = self.opt("whitespace")

op = self.opt("operator")
if op is None:
    return lhs

if op.text != "+":
    self.untok()
    return lhs

...
```

If we get an expression that looks like `1 + 2 ` (with a trailing whitespace) then the current implementation will use up that whitespace even though it doesn't belong to its AST node. Instead, we have to `untock` twice if there is a whitesace but no operator:

```py
ws = self.opt("whitespace")

op = self.opt("operator")
if op is None:
    if ws is not None:
        self.untok()
    return lhs

if op.text != "+":
    if ws is not None:
        self.untok()
    self.untok()
    return lhs

...
```

With the new checkpoint code, we don't have to think through any of this:

```py
try:
    with self.checkpoint():
        _ = self.opt("whitespace")

        op = self.expect("operator")
        if op.text != "+":
            raise ParseFailedError("expected <operator +>")
except ParseFailedError:
    return lhs

...
```

We automatically get the correct backtracking behavior regardless of how many tokens we have to consume. Note that the `op.text` check is also somewhat cleaner.

Just like `+`, the `=` token is actually a `<delimiter>` token with the constraint that its text is `=`, so we will now write `expect_op` and `expect_delim` helpers that will check both the token type and its text, and raise pretty errors in the cases of mismatch (if we use just an `expect` and an `if`, the errors from `expect` will not include the expected operator or delimiter text).

---

Finally, we are ready to write the parsing methods for `assignment_stmt` and other new nodes. To integrate it with the rest of the parser, we change `simple_stmt` to try the assignment first:

```py
@parse_function
def simple_stmt(self) -> SimpleStmt:
    try:
        with self.checkpoint():
            return SimpleStmt(type="simple_stmt", x=self.assignment_stmt())
    except ParseFailedError:
        return SimpleStmt(type="simple_stmt", x=self.expression_stmt())
```

Note that the order of operations matters here because parsing the first token of `a = 123` as a variable lookup is actually completely valid. A traditional parser generator could figure out that the rest of the document is not parseable and choose the right parse (depending on the underlying parser type), but even CPython's PEG parser is reliant on the correct order here based on [a note in the official grammar definition:](https://docs.python.org/3/reference/grammar.html)

```peg
# NOTE: assignment MUST precede expression, else parsing a simple assignment
# will throw a SyntaxError.
```

At this point `test_parse` should pass with `a = 123`.

# Interpreting Assignments

To do anything meaningful with assignments, we also need to be able to read variable values. We will add a new source string to our test suite:

```py
a = 123
a + 1
```

This is our first multi-line/multi-statement source string, so we need to fix some deficiencies after we add it to `test_lex.py`.

The first is that our `readline` function that we feed to `tokenize` is wrong and will cause the reference tokens to have incorrect line information (as if they are all on the first line). We fix it by using [`StringIO.readline`](https://docs.python.org/3/library/io.html#io.StringIO) instead of a custom implementation. The next issue is that our `newline` tokens are hacky—we patched them to reset `text` to an empty string to match the fake newline that CPython inserts at the end of strings. This is not correct for regular newlines so we rework the lexer:

1. we get rid of the hack in `Token.token_info()`,
2. we remove the matching `\n` injection hacks in the lexer and in `test_parse.py`, and
3. instead of injecting a literal `\n` at the end of each source string, we now keep track of whether the last token was a newline; if `endmarker` does not follow a newline, we output a ghost newline token (with an empty `text`) before the marker.

After some tuning to make sure the `start` and `end` of the tokens match up (we track the newlines as ending on the next line, but CPython says they are on the same), we pass the new test.

---

Now, we take on parsing. First we patch `file_input` to allow multiple statements:

```py
@parse_function
def file_input(self) -> FileInput:
    xs: list[Statement] = []
    while True:
        end = self.opt("endmarker")
        if end is not None:
            break
        xs.append(self.statement())

    return FileInput(type="file_input", xs=xs)
```

Next, we add the ability to include a token as the child of an `Atom` (for identifiers) and adjust its `to_ast`. Finally, we patch the parse function for `atom` to allow `identifier` tokens.

Now we pass the parsing and lexing tests for both assignments and variable use, and we can move on to interpreting.

---

When we originally built the interpreter test code, we only supported expressions, not statements. We have to rewrite much of the harness to now support both. We will still need a way to compare the results produced by our interpreter with `eval`. There are two ways of doing this, after the test program finishes running:

1. either check that the values of all variables match up,
2. or check the value of some expression.

We will do both, but only automate the second for now (we will simply print the variables but not compare them).

First, we switch our parser from `parse_expr` to `parse`. Then, we change the interpreter call from `eval` to `exec`. Now we need to extract our test expression, which we will say is in the last statement of the source, if it contains an expression at all. We have a few layers to unwrap to get to the statement type:

```py
l = Lexer(data=src)
p = Parser(lex=l)
ast_ours = p.parse()
i = Interpreter()

assert isinstance(ast_ours, FileInput)

assert isinstance(ast_ours.xs[-1], Statement)
if isinstance(ast_ours.xs[-1].x.xs[-1].x, ExpressionStmt):
    for stmt in ast_ours.xs[:-1]:
        i.exec(stmt)

    ours = i.eval(ast_ours.xs[-1].x.xs[-1].x.x)
else:
    i.exec(ast_ours)
    ours = None
```

When we find an `ExpressionStmt`, we execute all the preceding statements, then evaluate the final expression. If we don't, we simply execute the entire program and assume the result is `None`.

Now we deal with the reference evaluation. After we parse the source, we get a [`ast.Module`](https://docs.python.org/3/library/ast.html#ast.Module) back and check if the final statement is a [`ast.Expr`.](https://docs.python.org/3/library/ast.html#ast.Expr) If it isn't we simply evaluate the source string as normal. Otherwise, we need to evaluate everything but the last statement. We do this by calling [`compile`](https://docs.python.org/3/library/functions.html#compile) on a new `ast.Module` that we create out of the contents of the parse: `ast.Module(body=stmts[:-1])`, then `eval`ing the result as normal. For the final statement, we create an expression-statement `ast.Expression(body=stmts[-1].value)` and `compile` it with `mode="eval"`, then `eval` it as before.

To see variable values in the reference case, we look at the dictionary given to `eval` as `locals`. This will not be sufficient in the future as we introduce variable scopes but it works right now.

---

Finally, we implement the interpret functions. We add a `locals: dict[str, object]` to the interpreter state. This is, again, not going to work with variable scoping in the future, but works for now. `eval` has to change to support identifiers in `Atom` nodes by reading them by name from `self.locals` (don't forget to normalize to NFKD!). `exec` needs support for `AssignmentStmt`, which we implement by `eval`'ing the assignment value and saving it in `self.locals`.

The test now passes:

```py
$ python -m vpy.test_interpret
a = 123
a + 10

Evaluated:
133
Reference:
133
Matches

Locals:
{'a': 123}
Reference:
{'a': 123}
```

# Compiling Assignment

First, we add a `locals` field to the compiler class (as before, this will need a rework when we support scoping). This will map variable names to their _latest_ assignment register name. LLVM does not allow overriding variables at all (since it requires programs in [static single-assignment form](https://en.wikipedia.org/wiki/Static_single-assignment_form)) so we will need to keep track of new uses of the variable ourselves. Also this means that we need to generate unique variable names for each assignment to the same variable. We will do this intelligently by using the `%"{name}"` form in LLVM. The only character we need to escape is double quote (`"`) itself with its hex representation `\22`. We write a helper function to escape such strings, then we adjust `next_var` to take an optional prefix so our variables can have a pretty name but still be unique through the use of the global variable index counter. We will only use quoting if the autogenerated name needs it. Thankfully, LLVM provides a regex for valid identifiers that we can check against. Note that here we stop using LLVM's unnamed variables (`%{idx}`) since they _must_ occur in order (`%1` before `%2` etc.) or LLVM will throw an error. We instead use a similar `%.{idx}` form which does not have such restrictions but otherwise works identically.

The next step is to add identifier support to the `Atom` visitor, like in the interpreter. Then we add support for `AssignmentStmt`. The implementation computes the value as before, then uses a no-op bitcast to assign it to the variable register.

The final result:

```
$ python -m vpy.test_compile
define i64 @test() {
  ; a = 123
  %.2 = bitcast i64 123 to i64
  %a.1 = bitcast i64 %.2 to i64

  ; a + 10
  %.3 = bitcast i64 10 to i64
  %.4 = add i64 %a.1, %.3

  ret i64 %.4
}

test() = 133
```
