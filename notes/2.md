# Regex Lexer

To produce the next token, we are going to have a list of all token regexes (with associated token types) and run them in order starting at the end of the last token (or start of the data). Note that this gives precedence to tokens earlier in the list, but the grammar should be unambiguous anyway.

In cases of ambiguous grammar this approach can actually work quite well too. You can support even more complicated cases by having the parser drive the lexer by telling it which tokens are allowed or disallowed at a specific position based on context. The lexer could then exclude the disallowed regexes. This can actually produce better error messages in addition to supporting really complex lexing grammars similar to what is achievable with GNU `flex` which can use flags to enable/disable rules, but even more powerful since we can use the parser to set the flags rather than just the lexing programi itself.

First we will make two classes: `Pos` to encapsulated our source index, line, and column position tracking data; and `Token` to hold the token type, start, end, and text. We then make `Lexer.next` store a `Pos` for the current lexing position and return a `Token`.

We use `dataclass` with `kw_only=True` by default for all of our classes so we get reasonable `__str__` and `__repr__` implementations, but for `Pos` and `Token` we will write our own since we will be printing them a lot.

Then we define the list of tokens, which for now will be just `decinteger` from before. We will need to add whitespace and the `+` operator for our expression to lex fully.

In `next` we do the following:

1. Iterate over all defined tokens
2. Match the regex using
3. If found, remember the parse current position, then update it by examining the parsed string to handle line/column data
4. Emit the token or throw an error

We get the following:

```
decinteger@1:1#0'1'
...
RuntimeError: unknown token at 1:2#1
```

We will improve error output later, including adding a visual position indicator in the target line. This is fine for now.

Note that one tricky thing to get right is line/column updates. For the lines, you can simply count line terminators with a regex. For the column, you have to either a) add the length of the match, if there are no line endings or b) reset it to the number of characters after the last line end.

Remember to copy the end position before passing them to the token since we will modify it on the next iteration.

---

Now we need to parse whitespace, and `+`. [The spec](https://docs.python.org/3.13/reference/lexical_analysis.html#whitespace-between-tokens) says whitespace is `[ \t\f]+` between any tokens, except in a string, and at the beginning or end of line. Since we do not have lines or strings, we will just parse all `[ \t\f]+` sequences as whitespace for now.

Note that the built-in tokenizer does not emit whitespace. This in particular makes it hard to write programs that want to verify whitespace formatting rules or change the program without cloberring all the whitespace (like code mods/refactoring tools or certain permissive formatters). This is why we committed to emitting a Concrete Syntax Tree rather than an Abstract Syntax Tree.

The new result is:

```
decinteger@1:1#0'1'
whitespace@1:2#1' '
...
RuntimeError: unknown token at 1:3#2
```

Finally, we deal with operators. [The spec simply lists them as it does keywords.](https://docs.python.org/3.13/reference/lexical_analysis.html#operators) We use multi-cursor editing tricks to break down the table (careful with `-` in particular, as some IDEs treat it as word separator), and stuff everything in one regex. Note also that some operators are special symbols in regex, so we have to deal with them carefully. We could alternatively generate the regex from the operator table using `re.escape`. We also put the keywords to the same treatment.

New result:

```
decinteger@1:1#0'1'
whitespace@1:2#1' '
operator@1:3#2'+'
whitespace@1:4#3' '
decinteger@1:5#4'2'
...
RuntimeError: unknown token at 1:6#5
```

The last step is end-of-file handling. We will do the same thing as the CPython tokenizer and emit a `ENDMARKER` token at the end.

# Basic Testing

It would be nice to know how far our lexer is from parity with the CPython lexer so we will write a basic test harness that calls `tokenizer.tokenize` and our lexer and compares the result. We will also need to write a converter between our `Token` class and the built-in `TokenInfo` so we can compare them.

One thing we are missing from our `Token` that `TokenInfo` saves is the `line` field. We might want to simply ignore this for now by replacing it with an empty string in the reference `TokenInfo`s before comparing the tokens.

We add a `def token_info(self) -> TokenInfo` method to `Token`. We skip whitespace tokens, we map our type to the type integer from the `tokens` module. We turn our `Position` classes into tuples of line and column numbers. Note that `TokenInfo` counts columns from 0 and we count from 1.

Output:

```
decinteger@1:1#0'1'
whitespace@1:2#1' '
operator@1:3#2'+'
whitespace@1:4#3' '
decinteger@1:5#4'2'
endmarker@1:6#5''
>>>
TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='')
TokenInfo(type=55 (OP), string='+', start=(1, 2), end=(1, 3), line='')
TokenInfo(type=2 (NUMBER), string='2', start=(1, 4), end=(1, 5), line='')
TokenInfo(type=0 (ENDMARKER), string='', start=(1, 5), end=(1, 5), line='')
>>>
TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='')
TokenInfo(type=55 (OP), string='+', start=(1, 2), end=(1, 3), line='')
TokenInfo(type=2 (NUMBER), string='2', start=(1, 4), end=(1, 5), line='')
TokenInfo(type=4 (NEWLINE), string='', start=(1, 5), end=(1, 6), line='')
TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
```

Now we can feed this into `difflib` to see how close we got:

```diff
 TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='')
 TokenInfo(type=55 (OP), string='+', start=(1, 2), end=(1, 3), line='')
 TokenInfo(type=2 (NUMBER), string='2', start=(1, 4), end=(1, 5), line='')
-TokenInfo(type=0 (ENDMARKER), string='', start=(1, 5), end=(1, 5), line='')
+TokenInfo(type=4 (NEWLINE), string='', start=(1, 5), end=(1, 6), line='')
+TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
```

The only difference is that Python injects a newline at the end of the file if there isn't one there yet. This means that we not only need to do the same, but also learn to parse newlines. Technically `NEWLINE` is a token representing a logical line end, so we need to deal with line concatenation to parse them properly (and emit as whitespace the physical newlines which do not match a logical newline). But for now we will just parse all `\n|\r\n|\r` patterns as `newline` tokens.

Two notes:

1. `TokenInfo` wants the `newline` token to end on the same line it started, whereas our `count`-based line tracking makes it end on the next line
2. `TokenInfo` uses an empty string for the token text instead of the actual line ending sequence.

Now we get a clean diff.
