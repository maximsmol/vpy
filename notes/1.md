# Project Goal

The end results we hope to achieve is a Python interpreter and compiler, which can self-host (compile itself), written in Python. For self-hosting in particular we will need to also implement a Python runtime, in Python, so it can be compiled by the compiler into a native runtime. Note that this is a sort of recursive dependency since the runtime would need a runtime as it's written in Python. We need to cut it along some separation line by restricting it to a subset of Python which can be hosted without a runtime, perhaps by only using some built-in FFI machinery (e.g. `ctypes`) or by introducing our own FFI which works with the compiler internals (or both).

# Setting Up

We begin by setting up a new Python project and searching for some official resources to test our future implementations:

1. [The language specification](https://docs.python.org/3.13/reference/)
2. [An implementation test suite](https://github.com/python/cpython/tree/63a949487840f9b4b1d81df3aba06230ca8e5c28/Lib/test)

Note that in both cases we freeze the version to 3.13 to avoid things changing over time as the upstream language evolves.

# Early Goals

To get some momentum, we want to get some smoketests going as soon as possible. Our early goals are going to look something like this:

1. Lex a basic expression,
2. Parse a basic expression,
3. Interpret a basic expression,
4. Compile a basic expression.

We can breeze through steps 1 through 3, but 4 is going to take a lot longer. This is why we are also going to include an interpreter in the project. It will help us see results and test semantics before commiting to writing a machine code generator.

After we have some basic infrastracture that passes these goals we will set up the conformance test suite and start gradually beefing up all our layers to match spec and pass the tests.

Our basic expression for now will be `1 + 2`.

# Lexing

We are going to reference [the specification](https://docs.python.org/3.13/reference/lexical_analysis.html) to avoid building ourselves into a corner, but we will implement only the parts necessary to parse `1 + 2`.

Set up `__main__.py` to store a string `1 + 2` as the file source, create a `Lexer` class (from `lex.py`) and call `.next_token` on it in a loop.

There are a few things to consider immediately:

1. File encoding. Python's spec states that "program text [is] Unicode code points" and that the actual encoding "can be given by an encoding declaration and defaults to UTF-8" as specified is [PEP 3120.](https://peps.python.org/pep-3120/) Programs are thus required to be valid Unicode, even though its encoding may vary. Python's `str` holds UTF-8, which can store any valid Unicode string, so it can store any valid Python program, and we will be using it as our source buffer.

2. Location tracking. As is good practice, we are immediately going to build file offset, line, and column number tracking for all of our tokens.

3. Concrete syntax trees. We will want to make it possible to reconstruct the source file exactly from our various internal syntax representations. This allows for better/easier implementations of formatters and other language tools as they can keep and interpret whitespace and comments among other semantically unimportant tokens. On the lexer level this means we are going to carry around slices of the original program text inside of each token.

4. Streaming parsing. It can be useful for any parser (but especially for communication protocols and large binary formats) to allow for streaming the data—parsing data one buffer one at a time, rather than requiring all of it in memory at the same time. This allows us to parse data from a network stream, for example, or simply to reduce the maximum memory usage of our parser for any given file. We are going to keep this in mind to try and avoid choices that make streaming impossible in the future, but we will not build a streaming parser at first as it can greatly complicate the control flow of the implementation.

---

We start with a stub implementation of `next` that simply returns the string `<unk>`. Then we test that we have running code with `python -m vpy`. This is deliberately trivial, we want to make sure code behavior is as we expect at extremely frequent intervals.

Now we need to figure out what is the actual correct output of our lexer by reading the spec. It's best to also familiarize ourselves with [the basic core lexing concepts in Section 2.1](https://docs.python.org/3.13/reference/lexical_analysis.html#line-structure) as we go along. Indentation in particular is pretty complicated in Python.

One thing we do along the way is safe [the list of keywords from Section 2.3.1](https://docs.python.org/3.13/reference/lexical_analysis.html#keywords) These come in a table which is kind of hard to turn into source code, but it's possible with multi-cursor editing—place a cursor on the beginning of each line by dragging with Alt/Option or Control held (depending on your IDE), then using Alt/Option+Arrow Keys to move up by words until the second leftover column, then same with Shift to select the words, then paste all the new copied lines on a single cursor below the previously extracted keywords. Alternatively we could save the table as is and parse it in code, which would also make it easier to update in the future if the language spec changes. We do not bother with "soft" keywords for now.

Note that we could also use the Python standard library to get the keyword list, but we will minimize our usage of the built-in parser utilities as much as possible (since naturally we want to write all of this ourselves).

The first relevant token type for us is [the interger literal.](https://docs.python.org/3.13/reference/lexical_analysis.html#integer-literals) We are going to use regexes to parse the source code, so we translate the grammer into a series of variable definitions:

```py
nonzerodigit = r"([1-9])"
digit = r"([0-9])"
bindigit = ...
octdigit = ...
hexdigit = fr"({digit} | [a-fA-F])"

# integer = decinteger | bininteger | octinteger | hexinteger
decinteger = fr"""
(
  {nonzerodigit} (_? {digit})*
  |
  0+ (_? 0)*
)
"""
bininteger = fr"""
(
  0
  [bB]
  (_? {bindigit})+
)
"""
octinteger = ...
hexinteger = ...
```

We will later `re.compile` these using `re.VERBOSE`, which is why they have so much whitespace. This is generally advised for all non-trivial regexes as it makes them much more readable. In this case it also lets us match the non-whitespace-sensitive BNF grammar more closely. We want to use f-string interpolation to mimic the grammar so we leave the regexes as strings for now. Note that I also place parentheses around each definition to allow for treating them as a single unit when embedding. This is an old trick from the C preprocessor world. Consider the following example as motivation:

```py
foobar = r"foo|bar"
foobars = fr"{foobar}+"
# becomes r"foo|bar+"
assert re.compile(foobars).match("barrrrrr") is not None

# vs
foobar = r"(foo|bar)"
foobars = fr"{foobar}+"
# becomes r"(foo|bar)+"
assert re.compile(foobars).match("foobarfoofoobar") is not None
```

Since the regexes are quite long, we are going to split them out into files based on the token category. These go under `grammar/integer.py`.

Note that we are not concerned with the fact that e.g. `bininteger` is made of `bindigit`s since the lexer never emits structures with any sort of nesting. That being said, it could be an interesting research project to produce a lexer/regex engine which natively supports embedding sub-regexes and identifying where in the source code they matched. This should be relatively straight forward with NFA emulation and would further blur the distinction between lexing and parsing.

The one thing that we didn't handle yet is the `integer` token which is one of the integer syntaxes. We would probably benefit from preserving the exact form of integer parsed since this affects the numerical value, so we need to be able to define some sort of hierarchy of integers. For now, let's try to match the CPython tokenizer from the `tokenize` standard library module:

```py
ran = False

def readline() -> bytes:
    nonlocal ran

    if ran:
        return b""

    ran = True
    return b"1 + 2"

for t in tokenize.tokenize(readline):
    print(t)

# TokenInfo(type=65 (ENCODING), string='utf-8', start=(0, 0), end=(0, 0), line='')
# TokenInfo(type=2 (NUMBER), string='1', start=(1, 0), end=(1, 1), line='1 + 2')
# TokenInfo(type=55 (OP), string='+', start=(1, 2), end=(1, 3), line='1 + 2')
# TokenInfo(type=2 (NUMBER), string='2', start=(1, 4), end=(1, 5), line='1 + 2')
# TokenInfo(type=4 (NEWLINE), string='', start=(1, 5), end=(1, 6), line='1 + 2')
# TokenInfo(type=0 (ENDMARKER), string='', start=(2, 0), end=(2, 0), line='')
```

We see that actually even integer, float, and complex literals are all lumped together. We'll likely want to do better but we can revisit this in a second. Right now let's wire up our regex engine.
