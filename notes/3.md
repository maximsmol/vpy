# Lexing Test Suite

Before we move on from lexing, let's make a little test script (`test_lex.py`) to judge our progress in the future. A natural inclination is to lex the compiler text itself, but this would lead to tests that change very frequently. We are instead going to lex the standard library and, later, the CPython test suite (this is because the standard library is already installed and we don't want to figure out how to get the test suite right now).

We extract our testing code from `__main__.py` and rewrite it to work on files. We will specify specific modules to try to load for now. Our starting test will be the `tokenizer` module itself which we find with `import tokenizer; tokenizer.__file__`. We will, of course, fail to parse it at the very first token since it's an docstring and we do not yet parse strings.

I'm also adding an `assert` that verifies that there is no diff between our tokens and the CPython tokens for `1 + 2`.

# Starting Parsing

We are hand-writing a recursive descent parser. There are plenty of good alternatives: CPython uses PEG, a lot of languages use GNU Bison (which is LALR), parser combinators and Earley parsers are also interesting approaches. That being said, the most complicated compilers (C/C++) are all hand-written because they are not-quite-context-free gramars in practice. Hand-written parsers are also often easier to debug and can produce much better error messages, including by break away from being fully context-free.

We make `parse.py` with a `Parser` class that takes our lexer and will ask it for tokens. Again, we need to consult [the reference](https://docs.python.org/3.13/reference/expressions.html) to see what the AST is supposed to look like. We want to focus on just our simple binary expression again, so we reference the doc for expressions specifically. We'll mock out some of the general AST stuff as we go along as well. The focus is on [`a_expr` from binary arithmetic expressions.](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-a_expr)

We end up pulling in quite a stack of stuff—`a_expr`, [`m_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-m_expr) [`u_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-u_expr) [`power`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-power) [`primary`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-primary) [`atom`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-atom) [`literal`](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-literal)—before we hit our first token type: [`integer`](https://docs.python.org/3.13/reference/lexical_analysis.html#grammar-token-python-grammar-integer) (which we simplify to our only supported token [`decinteger`](https://docs.python.org/3.13/reference/lexical_analysis.html#grammar-token-python-grammar-decinteger) for now). We introduce an `expect` function that asserts that the next token is what we expect and throws otherwise. We will need to handle trying a parse without exploding later, for which we will need to keep a pushback stack of tokens to support look-aheads. The Python grammar probably allows for limited look-ahead but we can just go ahead and use a growing queue for it for convenience. This will also allow us to try-parse entire AST nodes rather than individual tokens.

Now we need to come up with some return value for our AST nodes. One thing of note is that we want to keep the list of tokens under each AST node to allow un-parsing the AST easily and uniformly. We have to decide whether a parent node will contain tokens from child nodes, and for now let's say no. This leads to a decision of how to make sure that child nodes can be inserted into the token stream when unparsing We want children to be named fields (e.g. `lhs` and `rhs` for binary operations), but this loses their position in the token stream. It can be recovered by looking at their tokens recursively to find the beginning of the node to position it. Alternatively, we can allow child nodes to occur in the token list, which is what we will do. This means each node knows its children and tokens both a) by position and b) by name (for named children).

We store the type of the AST node as a `str` type field for now (actually we use generics with a `str` bound so we can put `Literal["a_expr"]` in there). The alternative is to make a class for each AST node type, then use `isinstance` to check types. We will probably end up doing this eventually since we need to have alternative contents for the nodes while having them belong to the same type of node (e.g. `literal` might be an integer node or a string node).

Next step is to smoke-test the parser. We will simply parse one number and then let the parser exit, this way we don't yet have to deal with alternatives, lookahead, etc. The code is extremely simple:

```py
l = Lexer(data=src)
p = Parser(lex=l)
pprint(p.a_expr())

```

Note that `pprint` is not a typo. [This is a standard library pretty printer.](https://docs.python.org/3/library/pprint.html#pprint.pprint) We use it temporarily before we write a less verbose custom presentation for our AST nodes.

One interesting detail of our design is that it pulls tokens from the scanner on-demand, one at a time. This makes it ammendable to streaming parsing and to parser-guided scanning that were mention before. We are not really planning to use these features at all but it's worth noting that the overall approach could accommodate them.

---

The current presentation is difficult to read so we might as well fix it up right now. One major flaw is that since we store all child nodes in an array with the all the raw tokens, we end up with duplicates where children have named fields. Since all children are already in the array, we won't print the fields at all. However, we will include the field names in the representation. To find the field names, we are going to use the [`dataclasses.fields` API](https://docs.python.org/3/library/dataclasses.html#dataclasses.fields) which lets us introspect our AST node classes and is one of many great advantages of `dataclass`. We will use [`id`](https://docs.python.org/3/library/functions.html#id) to match up our nodes to the fields to avoid having to make them comparable or hashable. This places a requirement on properly reusing the same node in the parser code but it will be easy to follow because we produce nodes by consuming the input, which can only be done once without tricks.

Since we are not going to use the pretty printer anymore, we need to deal with our our indentation. We do this by passing around an `indent` keyword parameter, which is a string we prepend to all new lines. This is a somewhat cleaner approach than constructing the string each time from an indentation number. This means we also cannot directly override `__str__` (since we cannot call it with an argument). Instead, we are defining a `to_str` function and then aliasing `__str__` to it.

Our current version of the parse for `1` (remember we are not parsing anything but a decimal integer) is the following:

```
a_expr{
  m_expr{
    u_expr{
      power{
        #x: atom{
          literal{
            decinteger@1:1#0'1'
          }
        }
      }
    }
  }
}
```

Note only `power` has a named field right now for demonstration purposes.

---

Let's parse the rest of our addition expression now. We are going to remove the white space from the input so we can deal with it later. We are also not going to parse anything recursive like `1+2+3`. Just focus on the simplest case for now. We still have two problems:

1. we need to be able to look-ahead to check if a `+` sign follows our integer,
2. we need to include the `+` token in the AST node so we can preserve our un-parsing ability.

For look-ahead we add a stack to the class for pushing back a token if we want to retry parsing it. An alternative would be to physically push back the lexer position, which would be better if we had parser-guided lexing, but saving just the token is probably simpler. We end up with two functions: `tok` which returns the next token from the stack or from the lexer, and `untok` which places a token on the pushback stack so it becomes next in line. A simple use for this is an `opt` (short for "optional") function which calls `tok`, compares it with a given token type, and calls `untok` if there is a mismatch, and returns the `Token` or `None` depending on whether the type matched. We also rewrite `expect` to use `opt`.

The following is `a_expr` that recognizes addition using `opt`, manually tracking `children`:

```py
@dataclass(kw_only=True)
class AExpr(Node[Literal["a_expr"]]):
    lhs: Node[Literal["m_expr"]]
    rhs: Node[Literal["m_expr"]] | None

def a_expr(self) -> AExpr:
  lhs = self.m_expr()

  op = self.opt("operator")
  if op is not None:
      if op.text == "+":
          rhs = self.m_expr()
          return AExpr(type="a_expr", children=[lhs, op, rhs], lhs=lhs, rhs=rhs)

      self.untok(op)

  return AExpr(type="a_expr", children=[lhs], lhs=lhs, rhs=None)

```

And the resulting AST:

```
a_expr{
  #lhs: m_expr{
    u_expr{
      power{
        #x: atom{
          literal{
            decinteger@1:1#0'1'
          }
        }
      }
    }
  }
  operator@1:2#1'+'
  #rhs: m_expr{
    u_expr{
      power{
        #x: atom{
          literal{
            decinteger@1:3#2'2'
          }
        }
      }
    }
  }
}
```

One downside of having to manually track `children` is the repeated `AExpr(type="a_expr", children=[lhs, ...], lhs=lhs, rhs=...)` construction. We also have to not forget the `operator` token. It's pretty easy in this case since we have to check it anyway, but when we do not need to check the `text` of the token or whitespace gets involved we will start forgetting things.

What we will do, is keep all the tokens read by the parser and any AST nodes created in the parsing functions in a buffer, which will be attached to each AST node after it is done parsing. To track tokens, we modify `tok` and `untok` (which now takes the last recorded child from the buffer, checking that it is a `Token`). To track nodes and attach the children, we write a decorator called `parse_function` which we will use to annotate each parsing function returning an AST node. This decorator will peek at the `self` argument to get to the child array before passing all arguments to the decorated function, and passing its return value up the stack. It will use a `try-finally` to temporarily record `self.children`, then replace it with an empty list for the next level of the parse, then call the parse function, then attach the current `self.children` to its result, then restore the old `self.children` for the parent's parse.

The result is this version of `a_expr` and the exact same AST as before:

```py
@parse_function
def a_expr(self) -> AExpr:
    lhs = self.m_expr()
    rhs: Node[Literal["m_expr"]] | None = None

    op = self.opt("operator")
    if op is not None:
        if op.text == "+":
            rhs = self.m_expr()
        else:
            self.untok()

    return AExpr(type="a_expr", lhs=lhs, rhs=rhs)
```

---

Now we can tackle whitespace. We will again use the spaced `1 + 2` expression for the rest of the document.

[The spec states that it can occur anywhere unless removing it would result in a different token being formed from a concatenation of two other tokens.](https://docs.python.org/3.13/reference/lexical_analysis.html#whitespace-between-tokens) This means we basically only care about whitespace around keywords and multi-character operators. Note that we still have to _preserve_ all whitespace since we want to be able to reproduce the exact input source code. For that, we will simply reuse our automatic token tracking. What remains is to simply add optional (and, in the future, required) whitespace parses using `opt`, since we already collapse all whitespace into a single token:

```py
@parse_function
def a_expr(self) -> AExpr:
    lhs = self.m_expr()
    rhs: Node[Literal["m_expr"]] | None = None

    _ = self.opt("whitespace") # <<<

    op = self.opt("operator")
    if op is not None:
        if op.text == "+":
            _ = self.opt("whitespace") # <<<
            rhs = self.m_expr()
        else:
            self.untok()

    return AExpr(type="a_expr", lhs=lhs, rhs=rhs)
```

This then produces the following parse tree:

```
a_expr{
  #lhs: m_expr{
    u_expr{
      power{
        #x: atom{
          literal{
            decinteger@1:1#0'1'
          }
        }
      }
    }
  }
  whitespace@1:2#1' '
  operator@1:3#2'+'
  whitespace@1:4#3' '
  #rhs: m_expr{
    u_expr{
      power{
        #x: atom{
          literal{
            decinteger@1:5#4'2'
          }
        }
      }
    }
  }
}
```

---

To make this a complete parser outline, we need to parse down from a root node of some sort. In python this is [`file_input`,](https://docs.python.org/3.13/reference/toplevel_components.html#file-input) which then goes through [`statement`,](https://docs.python.org/3.13/reference/compound_stmts.html#grammar-token-python-grammar-statement) [`stmt_list`,](https://docs.python.org/3.13/reference/compound_stmts.html#grammar-token-python-grammar-stmt_list) [`simple_stmt`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-simple_stmt) [`expression_stmt`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-expression_stmt) [`starred_expression`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-starred_expression) [`or_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-or_expr) [`xor_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-xor_expr) [`and_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-and_expr) [`shift_expr`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-shift_expr) and finally reaches our `a_expr`.

Again a note on streaming parsers—we would need to produce the root node piece-by-piece. A natural solution could be to parse each statement one at a time but you could also do something complicated like stream partial AST node fields as they become concretely parsed, taking inspiration from various JSON streaming approaches.

The full file tree for `1 + 2` becomes:

```
file_input{
  statement{
    stmt_list{
      simple_stmt{
        expression_stmt{
          starred_expression{
            or_expr{
              xor_expr{
                and_expr{
                  shift_expr{
                    a_expr{
                      #lhs: m_expr{
                        u_expr{
                          power{
                            #x: atom{
                              literal{
                                decinteger@1:1#0'1'
                              }
                            }
                          }
                        }
                      }
                      whitespace@1:2#1' '
                      operator@1:3#2'+'
                      whitespace@1:4#3' '
                      #rhs: m_expr{
                        u_expr{
                          power{
                            #x: atom{
                              literal{
                                decinteger@1:5#4'2'
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

Note that there is also an unparsed `newline@1:6#5'\n'` token at the end. We will forbid leftover tokens in the future.
