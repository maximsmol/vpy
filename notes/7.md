# Lexing a Realistic Program

We will now start work on a more practical program. We will be using Problem 2.01 from [chapter 2 ("Arithmetic") of the 99 Prolog Problems set.](https://web.archive.org/web/20160817140043/https://sites.google.com/site/prologsite/prolog-problems/2) The task is simply to determine whether a given number is prime. We will take other problems from this set later.

We modify `test_lex.py` to load files from a test directory and write a Python solution for this problem using [the "trial divison" method.](https://en.wikipedia.org/wiki/Primality_test)

When we run the test, we see that our lexing is actually not that bad. We have a couple of bugs and lack support for indented blocks, but most of the program lexes fine!

The first issue is that we are parsing `->` from the type annotation as `-` and `>`. This is because operators (like `-`) currently take precedence over delimiters (like `->`). When we swap the order, we actually se a different problem. Now operator `==` is parsed as two delimiters `=` and `=`. What we really need is to prefer longer keywords over shorter ones. We split both the delimiters and the operators into subsets that are multi-character and single character and parse the long ones before the short ones so they take precedence.

---

The next problem is that we never emit `INDENT` or `DEDENT` tokens. We will gloss over a bunch of details here for now (such as tab handling).

First we detect whether we are at the start of a "logical line". Since we do not support line joining yet (either explicit with backslash `\` or implicit in parenthesized expressions), we simply check whether the token starts at column 1. Then, we add the indentation stack field described in the spec.

At each token, if we are at the start of the line (and the token is not a newline itself), we check the indent level. If this is a whitespace token, the level is derived from the text of the token, otherwise it is 0.

We compare the level with the top of the indentations stack as the spec describes and, if the new indentation level is greater, replace the `whitespace` with an `indent` (we will never have an indent at a non-whitespace token since they will all have a level of 0). If the level is equal, we do nothing as the spec requires.

If the level is less, then we must emit some number of dedent tokens. We will set up the lexer with a token stack to allow emitting multiple tokens. This allows us to schedule all the dedent tokens at once through a simple mechanism.

Now we need to deal with empty lines. Because of our new token stack, we can do this simply by using look-ahead. If the look-ahead is a `newline`, then the current line is empty and should not produce dedents. If it is anything else, dedents are required. In either case, we place the look-ahead token on the stack to be emitted later.

One more thing for `dedent`: before we finish the file, we need to empty the indentation stack. We will queue the `dedent`s in the same place we handle `endmarker`. The token stack again makes this super easy.

---

There is just one difference remaining between our token stream and the one produced by CPython. Sometimes the CPython `NEWLINE` tokens are instead emitted as `NL` tokens. This is part of the line joining mechanism, but in our tests so far only happens on empty lines. We add a new token type (since `newline` tokens are semnatically important in the spec) and adjust our code to emit it for newlines that are at the start of a line.

# Parsing/Interpreting/Compiling a Realistic Program

There are a lot of new constructs in our test program:

1. Function definition (+ type annotations)
2. If statements
3. Modulo
4. Comparison
5. Return statements
6. Booleans
7. While loops
8. Modifying assignment (`+=`)

To avoid getting stuck trying to build everything all at once, we will first define little smoketest examples for each of these and implement them one at a time.

## `10 % 6`

First: parsing. Modify `MExpr` in the same way as `AExpr` before: include an optional `MExpr` left-hand-side (remember, everything is left-associative) and an operator, rewrite `to_ast`, add a custom `unparse` which can insert parentheses. Then copy the implementation for `a_expr` but replace the operator with `%` instead of `+`.

Second: interpreting. Copy the implementation of `AExpr` for `MExpr`. Replace `+` with `%`.

Third: compiling. Copy the implementation of `AExpr` for `MExpr`, replace `add` with `urem`.

Done. That was very easy since we already built the infrastructure for all of these.

## `1 == 1`

First: parsing. The spec does us a disservice here by improperly specifying that `star_expression` uses `or_expr` as its only child. [In the reference grammar](https://docs.python.org/3/reference/grammar.html) we can see that `expression`-type children are also allowed. This means we build the [`or_test`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-or_test) [`and_test`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-and_test) [`not_test`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-not_test) and [`comparison`](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-comparison) nodes. Only the `comparison` node will do anything for now. The immediate impulse is to go with a left-recursive encoding like for other binary operators, but it would be incorrect since in Python `1 == 1 == 1` is not the same as `(1 == 1) == 1`. If it were, then the answer would be `False` as it would simplify to `True == 1`. We replace the target of `starred_expression` with `or_test` since it can represent a plain `or_expr` like before.

Second: interpreting. We add trivial handlers for all the new nodes. The comparison node starts by evaluating its left-hand-side then, if there are any operators, combines them with matching right-hand-side values and, in order, evaulates each against the previous value. If any of the comparisons fail, return `False`, otherwise return `True`.

Third: compiling. Very similar code to the interpreter. Use `icmp eq` to compare the integers. Carry an accumulator of type `i1` (single bit boolean) and use `and` to combine all the comparisons with it. Zero-extend the accumulator to `i64` with `zext` to make it compatible with our `i64` harness.

Again, pretty easy.

# `2 * 5`

First: parsing. Very simple: patch `to_ast` in `MExpr` to output `ast.Mult()` for operator `*`. Patch `m_expr` to allow `*` in the expression.

Second: interpreting. Equally trivial. Patch the `MExpr` handler to branch on the operator and do the right thing. Patch `AExpr` while we're at it.

Compiling: trivial like the last two. Exactly the same as the interpreter

# `a = 1 \n a += 2`

Parsing: add nodes for [`augmented_assignment_stmt`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-augmented_assignment_stmt) [`augtarget`,](https://docs.python.org/3.13/reference/simple_stmts.html#grammar-token-python-grammar-augtarget) [`expression_list`,](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-expression_list) and [`conditional_expression`.](https://docs.python.org/3.13/reference/expressions.html#grammar-token-python-grammar-conditional_expression) Implement parsing functions for them (they are straightforward), then change the parser for `simple_stmt` to try parsing the augmented assignment.

Interpreting: implement all the new node handlers. Nothing interesting.

Compiling: add an extra expression statement (just `a` will do) at the end to make sure we return some value. Implement all the new node handlers. The only detail worth noting is that we need to override the variable referenced in `self.locals` for the assignment to take effect.

# `True` and `False`

Parsing: we adjust the `atom` parser to check if it finds an identifier that matches one of the harcoded constants `True`, `False`, or `None`. In these cases we return a `AstLiteral` instead. We change the `AstLiteral.to_ast` function appropriately to support the new literals.

Interpreting: implement the new cases in the handler for `AstLiteral`.

Compiling: we will ignore `None` for now since we do not have boxed values yet (there are no pointers). `True` becomes `1`, `False` becomes `0`. We do not have support for different data types yet, in the future these can be `i1`.

---

The remaining pieces are all quite complicated:

1. Function definition (+ type annotations)
2. If statements
3. Modulo (DONE)
4. Comparison (DONE)
5. Return statements
6. Booleans (DONE)
7. While loops
8. Modifying assignment (`+=`) (DONE)

We will take them on gradually at a time through the next couple of chapters.
