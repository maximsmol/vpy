# Other Data Types

There are a number of ways to go from this milestone:

1. Support other simple fixed-size data types like `bool` and `float`
2. Support `None`
3. Introduce values requiring heap allocation like `str`
4. Add container types like `list` and `dict`
5. Implement proper function closures

One important commonality is that most of these require being able to distinguish between values of different types at runtime and/or at compiletime. For example, right now we emit values in LLVM IR as `i64`, while if we want to implement `float`, the correct type for those values is `double`.

For some code, it is enough to choose one or the other because the type is obvious, like in the following example:

```py
a: int = 123 # i64
b: float = 0.999 # double
```

But for many Python constructs, there is no one correct type:

```py
def f(x: int) -> int | float:
    if x == 5:
        return 123 # i64
    return 0.999 # double

val = f(7) # ???
```

This is because Python is a dynamically typed language, a fact which we have so far been able to ignore. Essentially, if we implement Python's semantics strictly, all values must use the same type, which must be able to represet any value in the language e.g. `123`, `0.999`, `True`, `[1, 2, 3]`, `None`, etc. The code that we emit must deal with all possible values because any value can be assigned to any variable.

Note that this doesn't mean that all values are treated identically, but rather that the storage type must be identical and the emitted code must be prepared for any value. For example, the following illustrates how values of different types are treated differently:

```py
>>> 1 + 1
2
>>> "a" + 1
Traceback (most recent call last):
  File "<python-input-5>", line 1, in <module>
    "a" + 1
    ~~~~^~~
TypeError: can only concatenate str (not "int") to str
```

One key point is that a lot of code will emit errors for the vast majority of possible values, which is a perfectly acceptable way to deal with a value. Crashing or undefined behavior, however, is not. We can imagine the following pseudocode for the plus operator:

```py
def plus(a, b):
    if isinstance(a, int):
        return plus_i64(a, b)

    if isinstance(a, str) and isinstance(b, str):
        return concatenate(a, b)

    raise TypeError("unsupported operand type(s) for +")
```

Since any value might be passed to a similar implicit (or a plain explicit) `isinstance` check at any time, we must always store its type information alongside the value itself. In LLVM IR, we will represent all values as structures like the following:

```llvm
%value = type {
  i64, ; Type tag, 0 = int, 1 = float, 2 = str, etc.
  [0 x i8] ; Data, array of bytes of unspecified (written as 0) length
}

; Specific examples:
%value_int = type {
  i64, ; = 0
  i64  ; Value
}

%value_float = type {
  i64, ; = 1
  double  ; Value
}

%value_str = type {
  i64, ; = 2
  i64, ; Length
  ptr  ; Data pointer
}
```

Since these structures have variable size (depending on the specific value stored inside), we must allocate them on the heap. This means that the LLVM IR type for all of our values becomes `ptr`.

It makes sense to make this transition towards dynamic typing next. For testing, we will use our example with the function that conditionally returns an int or a float.

## Lexing

We need to add support for [floating point literals.](https://docs.python.org/3.13/reference/lexical_analysis.html#floating-point-literals) We add the regexes for all the lexemes. The resulting standard token type is just `token.NUMBER`, same as for integers.

The floats must take precedence over integers since `10.0` could otherwise be lexed as `<decinteger 10> <operator .> <decinteger 0>`.

We also change the parser to support the new literals. `AstLiteral` gets logic to emit correct nodes for `floatnumber` tokens. The parser gets a new `expect_any` method, which generalizes `expect` to accept a set of acceptable tokens rather than only one. The old `expect` method now calls `expect_any`.

## Parsing

The first issue is that pipe operator (`|`) in the return type of the function. This should parse as an `OrExpr` node, which we already have a stub for. The corresponding standard AST node is a `ast.BinOp`. We can reuse the left-associative binary operation parsing logic that we had to figure out in part 4 when implementing addition.

## Interpreting

We start by just adding support for `floatnumber` under the `AstLiteral` handler. This works, but it's cheating since we are handing off all the hard parts of dynamically-typed values to the host Python interpreter. This is a detail we glossed over from the very beginning as we could not easily represent everything with integers as in the compiler. Now, we will correct this deficiency by reimplementing all values from basic primitives.

First, we define our value type as dicussed before:

```py
@dataclass(kw_only=True, eq=False)
class Value:
    type_id: Literal["None", "int", "bool", "float"]
    value: bytes

    @classmethod
    def from_none(cls) -> Self: ...
    @classmethod
    def from_int(cls, x: int) -> Self: ...
    @classmethod
    def from_bool(cls, x: bool) -> Self: ...
    @classmethod
    def from_float(cls, x: float) -> Self: ...

    def expect_none(self) -> None: ...
    def expect_int(self) -> int: ...
    def expect_bool(self) -> bool: ...
    def expect_float(self) -> float: ...

    def to_python(self) -> object:
        match self.type_id:
            ...
```

We add a set of `from_*` and `expect_*` methods to our primitive values that convert to and from the underlying Python value of the stored value so we can use the host interpreter for primitive calculations like integer addition and comparison. We also add a generic `to_python` method so the test harness can compare the results against the reference interpreter.

Then, we change the type of the value in `FunctionReturn`, `Scope.locals`, and the return type of `eval`. This will let the type checker guide us towards implementing the new dynamically-typed values correctly. We work carefully through the new type errors and check for places where asserts were used to assume the type of a value, as they now need updating. Also we make sure `if` and `while` statements do not use the truthiness of the value structure.

---

We run into a problem with our `Function` type since it's not clear how to serialize it into `bytes`. Specifically, the function bodies are currently represented by their AST nodes, which can include almost any other AST node as a child. This means that we would need to add a way to serialize every single AST node, which is a lot of work and would include a bunch of informatin unrelated to inrepretation, like token locations. Alternatively, we could come up with some alternative representation of the AST nodes. For example, real Python interpreters use bytecode for this purpose (although they typically include the source code in some form as well, for debugging).

For now, we will mimic the way the compiler deals with functions—we will lift each function to the global scope, generating a unique name for each function value that gets created. The function value will only carry this name, and the interpreter will lookup the function body by name when encountering a call expression. This is also kind of cheating, but we will allow it since the code comes out so similar to the compiler. Tho hold the functions, we make a new `Interpreter` class, which each `Scope` holds a reference to.

---

We run through the existing test suite to make sure nothing is broken after the transition.

## Compiling

Broadly speaking, we need to make the following changes:

1. Emit the value structure type specification
2. Change all values from `i64` to `ptr`
3. Change all value access to `getelementptr` + `load`

The struct in this case will use a `i64` as the `type_id` since using strings would be marginally more complicated. In the future, we might instead use pointers to static strings.

### `expect_*`

We define `expect_*` helpers which currently just assume the type of the structure is whatever is expected and place the data in a new local, converting it to an appropriate type e.g. `i64` for integers and `double` for floats. The structure element access syntax in LLVM IR is a bit confusing because it's so concise. Our expect functions look like the following:

```llvm
%{ptr} = getelementptr %struct.value, ptr %{x}, i64 0, i32 1
%{res} = load <type>, ptr %{ptr}
```

This should be read left to right. First, we specify the structure or array type, which tells us how the data is laid out. In our case it's the value structure. Then, we give the pointer to the data. The remaining arguments are indicies. First, `i64 0` says that we are getting the first structure pointed to by `%{x}` (basically, LLVM assumes that the pointer is to an array of structs to make the syntax more generic). Second, we give `i32 1` to get the second field of the struct i.e. the data field. Finally, we load the data from the data field pointer.

Also note that struct field indices _have to_ be `i32` or LLVM will throw a `invalid getelementptr indices` error.

If we had a structure in the data field, we could add an additional `getelementptr` which would use the new structure as the type, and the data pointer as the base pointer. This would let us easily calculate sub-members of the data field. We only have to do this because our data field is dynamically typed. If you have nested structures (or arrays) with statically known types, you can keep chaining in the original `getelementptr`.

---

If the value type does not match what is expected, we will just abort the program for now. We tell LLVM to load the `puts` and `abort` symbols from the currently loaded dynamic libraries (which should include libc) with function declarations. Then, we check the type ID in each `expect_*` method and, if it is incorrect, we `puts` a constant error string and `abort()`.

### `make_*`

The `make_*` functions wrap primitive values into the value struct. The naive implementation would be to use `alloca` and place the structs on the stack, as we did for function arguments in part 10. However, since values are returned from functions, the stack-allocated structures could get clobbered before the function result can be read. This is exactly what we observe in practice since after the compiled code runs, the Python interpreter running the test harness destroys all the results and reads garbage data.

Instead, we use `malloc` to place all the values on the heap. We do not bother freeing the memory and simply let it leak. In the future, we will build a garbage collector to fix this.

One annoyance with `malloc` vs `alloca` is that we need to manually calculate the size of the structures that we allocate. There is a trick to get around this, however. We can use `getelementptr`, with `null` (i.e. `0`) as the base pointer, to calculate the size of any given LLVM type:

```llvm
%{size_ptr} = getelementptr {x}, ptr null, i64 1
%{res} = ptrtoint ptr %{size_ptr} to i64
```

### Wrapping Up

Then, we change all the existing compiler output to use `ptr` types and the `extract` and `make` functions where appropriate. This is pretty straightforward. One interesting note is that we are no longer forced to represent booleans and integers the same way, so we can change booleans to `i8` instead. We will not use `i1` for them because we need to read them in the Python test harness and it is unclear how `i1` would be represented in `ctypes`.

If we run the test harness right now, it will output a meaningless number since it will receive a raw pointer from the compiled code. We need to write a function that loads the data from the pointer and converts it into Python values. We do this by using `ctypes` and by reading the type tag to determine the actual data layout.

---

There is a failing test case which throws a `phi`-related error for the same reason as the `while` loop in the last chapter—the type checking code is now adding a block inside the if statement (in the conditional and in the `then` branch) and the generated phi nodes again do not expect it. Same as last time, we fix it by generating an "if-then epilog" block for every if statement, as well as an "if-cond epilog" block.
